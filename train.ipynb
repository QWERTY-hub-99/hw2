{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **RESNET34**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers,optimizers,datasets,Sequential\nimport os\nimport numpy as np\nimport datetime\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nlen(tf.config.experimental.list_physical_devices('GPU'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 数据读取","metadata":{}},{"cell_type":"code","source":"def preprocess(x,y):\n    x=2 * tf.cast(x,dtype=tf.float32)/255.-1\n    y=tf.cast(y,dtype=tf.int32)\n    return x,y\n\n(x,y),(x_test,y_test) = datasets.cifar100.load_data()\nx = x.astype(np.float32)\nx_test = x_test.astype(np.float32)\ny=tf.squeeze(y,axis=1)\ny_test=tf.squeeze(y_test,axis=1)\n\nbatch_size = 50\n\ntrain_db=tf.data.Dataset.from_tensor_slices((x,y))\ntrain_db=train_db.map(preprocess).batch(batch_size)\n\ntest_db=tf.data.Dataset.from_tensor_slices((x_test,y_test))\ntest_db=test_db.map(preprocess).batch(batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 数据增强","metadata":{}},{"cell_type":"code","source":"def cutout_mask(img,label):\n    length = 16\n    img = img.numpy()\n    label = label.numpy()\n  \n    batch_size,h,w,channel_num = img.shape\n    for i in range(0,batch_size,2):\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - length // 2, 0, h)\n        y2 = np.clip(y + length // 2, 0, h)\n        x1 = np.clip(x - length // 2, 0, w)\n        x2 = np.clip(x + length // 2, 0, w)\n        \n        img[i, y1: y2, x1: x2, :] = 0 \n    \n    return img, label\n\ndef mixup_mask(img,label):\n    img = img.numpy()\n    label = label.numpy()\n    batch_size,h,w,channel_num = img.shape\n    \n    for i in range(0,batch_size,2):\n        mixup_idx = np.random.randint(0,batch_size)\n        lamda = np.random.uniform()\n        img[i,:,:,:] = lamda*img[i,:,:,:] + (1-lamda) * img[mixup_idx,:,:,:]\n        label[i,:] = lamda*label[i,:] + (1-lamda) * label[mixup_idx,:]\n    return img,label\n\ndef cutmix_mask(img,label):\n    length = 16\n    img = img.numpy()\n    label = label.numpy()\n    batch_size,h,w,channel_num = img.shape\n    for i in range(0,batch_size,2):\n        mixup_idx = np.random.randint(0,batch_size)\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - length // 2, 0, h)\n        y2 = np.clip(y + length // 2, 0, h)\n        x1 = np.clip(x - length // 2, 0, w)\n        x2 = np.clip(x + length // 2, 0, w)\n        \n        lamda = 1 - (y2-y1)*(x2 - x1)/(h*w)\n        img[i, y1: y2, x1: x2, :] = img[mixup_idx, y1: y2, x1: x2, :]\n        label[i,:] = lamda*label[i,:] + (1-lamda) * label[mixup_idx,:]\n    return img,label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ResNet构建","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,Sequential\n\nclass BasicBlock(layers.Layer):\n    #初始化函数\n    #filter_num:理解为卷积核通道的数目，也就是channel的通道数\n    #stride = 1意味着对图片不进行采样\n    def __init__(self,filter_num,strides=1):\n        #调用母类的初始化方法\n        super(BasicBlock,self).__init__()\n        #filter_num：卷积核通道的数目.(3,3):卷积核的size\n        #padding='same'如果stride等于1，那么输出等于输入。\n        #如果stride大于等于2的话，padding=same,会自动补全，\n        # 如果等于2的话，输入是32x32,可能输出是14x14,那么如果padding=same\n        #会padding输入的大小，使得输出是16x16\n\n\n        self.conv1=layers.Conv2D(filter_num,(3,3),strides=strides,padding='same')\n        self.bn1=layers.BatchNormalization()\n        #非线性激活函数\n        self.relu=layers.Activation('relu')\n\n        #那么这里设置stride=1,就始终保持一样\n        self.conv2=layers.Conv2D(filter_num,(3,3),strides=1,padding='same')\n        self.bn2=layers.BatchNormalization()\n\n        if strides != 1:\n            #下采样\n            self.downsample=Sequential()\n            self.downsample.add(layers.Conv2D(filter_num,(1,1),strides=strides))\n        else:\n            self.downsample=lambda x:x\n\n\n\n    def call(self,inputs,training=None):\n        #[b,h,w,c]\n        out=self.conv1(inputs)\n        out=self.bn1(out)\n        out=self.relu(out)\n\n        out=self.conv2(out)\n        out=self.bn2(out)\n\n        identify=self.downsample(inputs)\n        output=layers.add([out,identify])\n        #使用tf的函数功能\n        output=tf.nn.relu(output)\n\n        return output\n\n\nclass ResNet(keras.Model):\n    def __init__(self,layer_dims,num_classes=100):\n        #layer_dims:resnet18里面有[2,2,2,2]，也就是四个resblock\n        #这里指定了一共有多少个resblock层，每个层有多少个basicblock\n        #后面在设置blocks的数量的时候，就是用的这里的层的个数\n      #一个resblock里面包含了两层basicblock\n        #num_classes = 100:就是我们设置的输出的类的个数\n        super(ResNet, self).__init__()\n\n        #实现预处理层\n        self.stem=Sequential([layers.Conv2D(64,(3,3),strides=(1,1)),\n                              layers.BatchNormalization(),\n                              layers.Activation('relu'),\n                              #layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding='same')\n                              ])\n        #创建4个res_block\n        #这里blocks的数量是layer_dims[0]\n        #这里创建的四个res_block与前面的layer_dims:[2,2,2,2]对应\n        #将stride设置为2是为了让feature_size越来越小\n        self.layer1=self.build_resblock(64,layer_dims[0])\n        self.layer2=self.build_resblock(128,layer_dims[1],strides=2)\n        self.layer3=self.build_resblock(256,layer_dims[2],strides=2)\n        self.layer4=self.build_resblock(512,layer_dims[3],strides=2)\n\n\n        #out:[b,512,h,w]\n        #经过运算之后不能得到h和w的值，\n        #使用自适应的方法得到h,w\n        #GlobalAveragePooling2D:就是不管你的长和宽是多少\n        #会在某个channel上面的长和宽加起来，取均值\n        self.avgpool=layers.GlobalAveragePooling2D()\n        #创建全连接层\n        #这里的Dense是用来分类的,这里输出是之前输出的类别，num_classes\n        #self.flatten = layers.Flatten()\n        self.fc=layers.Dense(num_classes,activation = 'softmax')\n\n\n\n    def call(self,inputs,training=None):\n        #完成前向运算过程\n        x = self.stem(inputs)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        #这里已经变成[b,c]的shape,不需要reshape了\n        x=self.avgpool(x)\n        #这里输出是[b,100]\n        x=self.fc(x)\n\n        return x\n\n\n\n    def build_resblock(self,filter_num,blocks,strides=1):\n        res_blocks=Sequential()\n        #添加第一层basicblock\n        #可能有下采样的功能的\n        res_blocks.add(BasicBlock(filter_num,strides))\n        #但是对于后面的basicblock不让有下采样功能\n        #从1开始，一直到blocks个\n        for _ in range(1,blocks):\n            #这样只会在第一个下采样，后面的不在下采样，保持shape不变\n            res_blocks.add(BasicBlock(filter_num,strides=1))\n        return res_blocks\n\ndef resnet18():\n    return ResNet([2,2,2,2])\n\ndef resnet34():\n    return ResNet([3, 4, 6, 3])  #4个Res Block，第1个包含3个Basic Block,第2为4，第3为6，第4为3\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型构建","metadata":{}},{"cell_type":"code","source":"model = resnet34()\nmodel.build(input_shape=(None,32,32,3))\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '_baseline'\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 训练","metadata":{}},{"cell_type":"code","source":"lr = 1e-3\noptimizer=optimizers.Adam(lr=lr)\n\ntrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\ntest_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n\nfor epoch in range(50):\n#     if epoch % 5 == 4:\n#         lr/=10\n#         optimizer.lr = lr\n    for step,(x,y) in enumerate(train_db):\n        #这里做一个前向循环,将需要求解梯度放进来\n        with tf.GradientTape() as tape:\n            y_onehot=tf.one_hot(y,depth=100)\n            logits=model(x)\n            #compute loss\n            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n            loss=tf.reduce_mean(loss)\n            \n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n        acc = correct/x.shape[0]\n        \n        train_loss(loss)\n        train_accuracy(acc)\n        #计算gradient\n        grads=tape.gradient(loss,model.trainable_variables)\n        #传给优化器两个参数：grads和variable，完成梯度更新\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch,step,'losses:',float(loss))\n            \n    \n    with train_summary_writer.as_default():\n        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n        \n    total_num=0\n    total_correct=0\n    for x,y in test_db:\n        logits=model(x)\n        y_onehot = tf.one_hot(y,depth = 100)\n        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n        loss=tf.reduce_mean(loss)\n        \n        #prob=tf.nn.softmax(logits,axis=1)\n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n\n        total_num += x.shape[0]\n        total_correct += int(correct)\n        \n        test_accuracy(correct/x.shape[0])\n        test_loss(loss)\n    \n    with test_summary_writer.as_default():\n        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n            \n\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch+1,\n                         train_loss.result(), \n                         train_accuracy.result()*100,\n                         test_loss.result(), \n                         test_accuracy.result()*100))\n\n    \n    train_loss.reset_states()\n    test_loss.reset_states()\n    train_accuracy.reset_states()\n    test_accuracy.reset_states()\n\nmodel.save_weights('baseline.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CUTOUT","metadata":{}},{"cell_type":"code","source":"model = resnet34()\nmodel.build(input_shape=(None,32,32,3))\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_cutout'\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n\nmodel.build(input_shape=(None,32,32,3))\nlr = 1e-4\noptimizer=optimizers.Adam(lr=lr)\n\ntrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\ntest_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n\nfor epoch in range(50):\n    # if epoch % 5 == 4:\n    #     lr/=10\n    #     optimizer.lr = lr\n    for step,(x,y) in enumerate(train_db):\n        #这里做一个前向循环,将需要求解梯度放进来\n        with tf.GradientTape() as tape:\n            y_onehot=tf.one_hot(y,depth=100)\n            x, y_onehot = cutout_mask(x,y_onehot)\n            #[b,32,32,3] => [b,100]\n            logits=model(x)\n            #[b] => [b,100]\n            #compute loss\n            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n    \n            loss=tf.reduce_mean(loss)\n            \n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n        acc = correct/x.shape[0]\n        \n        train_loss(loss)\n        train_accuracy(acc)\n        #计算gradient\n        grads=tape.gradient(loss,model.trainable_variables)\n        #传给优化器两个参数：grads和variable，完成梯度更新\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch,step,'losses:',float(loss))\n            \n    \n    with train_summary_writer.as_default():\n        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n        \n    total_num=0\n    total_correct=0\n    for x,y in test_db:\n        logits=model(x)\n        y_onehot = tf.one_hot(y,depth = 100)\n        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n        loss=tf.reduce_mean(loss)\n        \n        #prob=tf.nn.softmax(logits,axis=1)\n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n\n        total_num += x.shape[0]\n        total_correct += int(correct)\n        \n        test_accuracy(correct/x.shape[0])\n        test_loss(loss)\n    \n    with test_summary_writer.as_default():\n        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n            \n\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch+1,\n                         train_loss.result(), \n                         train_accuracy.result()*100,\n                         test_loss.result(), \n                         test_accuracy.result()*100))\n\n    \n    train_loss.reset_states()\n    test_loss.reset_states()\n    train_accuracy.reset_states()\n    test_accuracy.reset_states()\n\nmodel.save_weights('cutout.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MIXUP","metadata":{}},{"cell_type":"code","source":"model = resnet34()\nmodel.build(input_shape=(None,32,32,3))\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_mixup'\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n\nmodel.build(input_shape=(None,32,32,3))\nlr = 1e-4\noptimizer=optimizers.Adam(lr=lr)\n\ntrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\ntest_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n\nfor epoch in range(50):\n    # if epoch % 5 == 4:\n    #     lr/=10\n    #     optimizer.lr = lr\n    for step,(x,y) in enumerate(train_db):\n        #这里做一个前向循环,将需要求解梯度放进来\n        with tf.GradientTape() as tape:\n            y_onehot=tf.one_hot(y,depth=100)\n            x, y_onehot = mixup_mask(x,y_onehot)\n            #[b,32,32,3] => [b,100]\n            logits=model(x)\n            #[b] => [b,100]\n            #compute loss\n            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n    \n            loss=tf.reduce_mean(loss)\n            \n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n        acc = correct/x.shape[0]\n        \n        train_loss(loss)\n        train_accuracy(acc)\n        #计算gradient\n        grads=tape.gradient(loss,model.trainable_variables)\n        #传给优化器两个参数：grads和variable，完成梯度更新\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch,step,'losses:',float(loss))\n            \n    \n    with train_summary_writer.as_default():\n        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n        \n    total_num=0\n    total_correct=0\n    for x,y in test_db:\n        logits=model(x)\n        y_onehot = tf.one_hot(y,depth = 100)\n        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n        loss=tf.reduce_mean(loss)\n        \n        #prob=tf.nn.softmax(logits,axis=1)\n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n\n        total_num += x.shape[0]\n        total_correct += int(correct)\n        \n        test_accuracy(correct/x.shape[0])\n        test_loss(loss)\n    \n    with test_summary_writer.as_default():\n        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n            \n\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch+1,\n                         train_loss.result(), \n                         train_accuracy.result()*100,\n                         test_loss.result(), \n                         test_accuracy.result()*100))\n\n    \n    train_loss.reset_states()\n    test_loss.reset_states()\n    train_accuracy.reset_states()\n    test_accuracy.reset_states()\n\nmodel.save_weights('mixup.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CUTMIX","metadata":{}},{"cell_type":"code","source":"model = resnet34()\nmodel.build(input_shape=(None,32,32,3))\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_cutmix'\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n\nmodel.build(input_shape=(None,32,32,3))\nlr = 1e-4\noptimizer=optimizers.Adam(lr=lr)\n\ntrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\ntrain_accuracy = tf.keras.metrics.Mean('train_accuracy', dtype=tf.float32)\ntest_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\ntest_accuracy = tf.keras.metrics.Mean('test_accuracy', dtype=tf.float32)\n\nfor epoch in range(50):\n    # if epoch % 5 == 4:\n    #     lr/=10\n    #     optimizer.lr = lr\n    for step,(x,y) in enumerate(train_db):\n        #这里做一个前向循环,将需要求解梯度放进来\n        with tf.GradientTape() as tape:\n            y_onehot=tf.one_hot(y,depth=100)\n            x, y_onehot = cutmix_mask(x,y_onehot)\n            #[b,32,32,3] => [b,100]\n            logits=model(x)\n            #[b] => [b,100]\n            #compute loss\n            loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n    \n            loss=tf.reduce_mean(loss)\n            \n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n        acc = correct/x.shape[0]\n        \n        train_loss(loss)\n        train_accuracy(acc)\n        #计算gradient\n        grads=tape.gradient(loss,model.trainable_variables)\n        #传给优化器两个参数：grads和variable，完成梯度更新\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch,step,'losses:',float(loss))\n            \n    \n    with train_summary_writer.as_default():\n        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n        \n    total_num=0\n    total_correct=0\n    for x,y in test_db:\n        logits=model(x)\n        y_onehot = tf.one_hot(y,depth = 100)\n        loss=tf.losses.categorical_crossentropy(y_onehot,logits)\n        loss=tf.reduce_mean(loss)\n        \n        #prob=tf.nn.softmax(logits,axis=1)\n        pred=tf.argmax(logits,axis=1)\n        pred=tf.cast(pred,dtype=tf.int32)\n        correct=tf.cast(tf.equal(pred,y),dtype=tf.int32)\n        correct=tf.reduce_sum(correct)\n\n        total_num += x.shape[0]\n        total_correct += int(correct)\n        \n        test_accuracy(correct/x.shape[0])\n        test_loss(loss)\n    \n    with test_summary_writer.as_default():\n        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n            \n\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch+1,\n                         train_loss.result(), \n                         train_accuracy.result()*100,\n                         test_loss.result(), \n                         test_accuracy.result()*100))\n\n    \n    train_loss.reset_states()\n    test_loss.reset_states()\n    train_accuracy.reset_states()\n    test_accuracy.reset_states()\n\nmodel.save_weights('cutmix.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}